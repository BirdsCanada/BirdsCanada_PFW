[["index.html", "Project FeederWatch: A Data Users Guide ", " Project FeederWatch: A Data Users Guide Danielle Ethier Version 1: March 2022 Project FeederWatch is a continental citizen science dataset of winter bird distributions and abundances. The purpose of this workbook is to provide you with the basic tools and skills you will need to get started using the raw PFW data in R. Lets get started! "],["Intro1.html", "Chapter 1 Introduction 1.1 Project FeederWatch 1.2 Goal 1.3 Getting Involved 1.4 Prerequisites 1.5 Acknowledgements", " Chapter 1 Introduction 1.1 Project FeederWatch Welcome to Project FeederWatch! Since 1987, the Cornell Lab of Ornithology and Birds Canada (formerly Bird Studies Canada) have partnered on Project FeederWatch to mobilize thousands of citizen scientists across North America to count birds in their backyards over the winter. Specifically, FeederWatch is a winter-long (November-April) survey of birds that visit feeders at backyards, nature centers, community areas, and other locales. Participants periodically count the birds they see at their feeders and send their counts to Project FeederWatch. Decades of data provide a comprehensive look at continental wintertime populations of feeder birds over the late 20th and early 21st centuriesincluding some compelling stories of range expansions and contractions, populations in flux, and birds adapting to changing environments. 1.2 Goal Our goal of this online handbook is to demonstrate how to use the R statistical programming language (https://www.r-project.org/) to import, clean, and explore raw Project FeederWatch (PFW) data through visualizations and summaries, and run various analytic procedures. We hope the contents will be useful to researchers interested in PFW data. While this handbook is focused on Canadian PFW data, it could easily be used for US data with minor changes to the R code examples. If you have suggestions for additional examples, please let us know by emailing dethier@birdscanada.org. If you have specific questions about Project FeederWatch, please reach out to our Program Leads: Canada: pfw@birdscanada.org U.S: feederwatch@cornell.edu 1.3 Getting Involved Project FeederWatch is supported almost entirely by its participants. The annual participation fees cover materials, staff support, web design, data analysis, and the year-end report (Winter Bird Highlights). Without the support of our participants, this project wouldnt be possible. As a program that engages participants across the US and Canada, we strive to ensure that Project FeederWatch is accessible and welcoming to every person. FeederWatch is conducted by people of all skill levels and backgrounds, including children, families, individuals, classrooms, retired persons, youth groups, nature centers, and bird clubs. Please join the project for the country in which you reside: Canada or U.S Thank you for your contribution and participation! 1.4 Prerequisites This book assumes that you have a basic understanding of R. Regardless of whether you are new to R or not, we highly recommend that you become familiar with R for Data Science by Garrett Grolemund and Hadley Wickham (http://r4ds.had.co.nz/). Their book covers how to import, visualize, and summarize data in R using the tidyverse collection of R packages (https://www.tidyverse.org/). It also provides an invaluable framework for organizing your workflow to create clean, reproducible code (http://r4ds.had.co.nz/workflow-projects.html). We follow their lead by, wherever possible, using the tidyverse framework throughout this book. 1.5 Acknowledgements Project FeederWatch is a joint project of the Cornell Lab of Ornithology and Birds Canada. Project FeederWatch is sponsored in the U.S. and Canada by Wild Birds Unlimited and in Canada by Armstrong Bird Food. Many people have contributed to the success of PFW including its founder Erica Dunn, who also designed its protocol. Thank you to David Bonter and Emma Greig at Cornell and Kerrie Wilcox at Birds Canada for organizing and managing PFW over the years. Thanks to David Bonter, Emma Greig, Denis Lepage for managing the Feederwatch database. The text in this document was adapted from The Cornell Lab Project FeederWatch webpage. Funding provided by Environment and Climate Change Canada made the publication of this online resource possible. Thank you! "],["Start2.html", "Chapter 2 Data Manipulation 2.1 Installing packages 2.2 Working Directory 2.3 Data download and filtering", " Chapter 2 Data Manipulation In this Chapter, we will import the raw data and wrangle it into a format suitable for creating summary statistics. 2.1 Installing packages The functionality of some packages may require updated versions of R and RStudio. To avoid errors, please ensure you are using the most recent releases of R and RStudio, and update your R packages. update.packages() # may need to run this as an R administrator if you run into &#39;permission denied&#39; issues You need to load the package each time you open a new R session. #If you need to install the package, use the following script before loading the package. You only need to install the package once on your computer, and then it will always be available. For example: #install.package(&quot;tidyverse&quot;) require(tidyverse) require(lubridate) require(data.table) require(reshape) require(tibble) 2.2 Working Directory First, lets set up your working directory. If you dont know what your current working directory is you can check with the following script. getwd() If you want to save your data in a particular location on your computer, set your directory with the following script. #This is just an example that will need to be modified to the path on your computer setwd(&quot;C:/Users/username/Documents/foldername/&quot;) Alternatively, you can set the working directory using the Session tab &gt; Set Working Directory &gt; Choose Directory Now we will a create folder to save the raw data and store any outputs. We will also set shortcuts to these folders for future use. dir.create(&quot;Data&quot;) dir.create(&quot;Output&quot;) out.dir &lt;- paste(&quot;Output/&quot;) dat.dir &lt;- paste(&quot;Data/&quot;) 2.3 Data download and filtering Researchers seeking to conduct formal analyses using PFW data are invited to download the raw data from the Cornell Lab PFW website. As with the use of any data set, knowing the data structure, understanding the metadata, grasping the data collection protocols, and being cognizant of the unique aspects of the program are all critical for conducting analyses and interpreting results in ways that provide meaningful insights. Although the data are freely available, we invite researchers to consult with researchers at the Cornell Lab of Ornithology or Birds Canada to ensure that the data are being handled and analyzed in a meaningful way. Save the raw data in the Data folder you created in the previous step. In this example, we are going to work with all the Canadian PFW data. This is a big dataset, so we will process it in batches, otherwise R gets cranky. Step 1: Load Data Now that you have all the raw data saved in your Data folder, we are going to process each file sequentially to extract what we need. Each time you load a new dataframe you will overwrite the old canada.pfw dataframe. Notice we are using fread instead of read.csv. It is faster for big data files. #Process sequentially. Load one file and move to Step 2. #Old data canada.pfw&lt;-fread(&quot;Data/PFW_1988_1995_public.csv&quot;) canada.pfw&lt;-fread(&quot;Data/PFW_1996_2000_public.csv&quot;) canada.pfw&lt;-fread(&quot;Data/PFW_2001_2005_public.csv&quot;) canada.pfw&lt;-fread(&quot;Data/PFW_2006_2010_public.csv&quot;) canada.pfw&lt;-fread(&quot;Data/PFW_2011_2015_public.csv&quot;) canada.pfw&lt;-fread(&quot;Data/PFW_2016_2020_public.csv&quot;) #New data canada.pfw&lt;-fread(&quot;Data/PFW_2021_public.csv&quot;) Step 2: Filter Data Now that you have loaded your first dataframe filled with PFW data. Now we are going to filter and clean the data to ensure it is ready for future processing and analysis. Change the uppercase headers to lower case, since it appears that there is a mix of both, depending on the year. names(canada.pfw)&lt;-tolower(names(canada.pfw)) Filter out the U.S. data and retain the Canadian data only. I also remove unwanted data columns. You can change this code to filter for just U.S. data, or to remove/ maintain any data column you would like for your analytical purposes. canada.pfw &lt;- canada.pfw %&gt;% filter(subnational1_code %in%c(&quot;CA-ON&quot;,&quot;CA-SK&quot;,&quot;CA-BC&quot;,&quot;CA-QC&quot;,&quot;CA-MB&quot;,&quot;CA-AB&quot;,&quot;CA-NB&quot;,&quot;CA-NS&quot;,&quot;CA-NL&quot;,&quot;CA-PE&quot;,&quot;CA-YT&quot;,&quot;CA-NT&quot;)) %&gt;% dplyr::select(-entry_technique, -data_entry_method) %&gt;% collect() Here I separate the provincial code into its own column and remove pfw from the Period id canada.pfw&lt;-canada.pfw %&gt;% separate(subnational1_code, c(&quot;del1&quot;, &quot;Prov&quot;), sep=&quot;-&quot;, remove=FALSE) %&gt;% dplyr::select (-del1, -subnational1_code) %&gt;% separate(proj_period_id, c(&quot;del2&quot;, &quot;Period&quot;), sep=&quot;_&quot;, remove=FALSE) %&gt;% dplyr::select(-del2, -proj_period_id) To eliminate biases created by extending the PFW season in some years, data are truncated to after Nov 8 (doy=312) (the earliest possible 2nd Saturday) and end Apr 3 (doy=93) (earliest possible end date). canada.pfw &lt;- canada.pfw %&gt;% mutate(date = ymd(paste(year, month, day, sep = &quot;/&quot;)), doy = yday(date)) %&gt;% filter(doy &lt;= 93 | doy &gt;= 312) Now we assign a floor.week variable which starts on Saturday (i.e., +6) canada.pfw &lt;- canada.pfw %&gt;% mutate(floor.week=floor_date(canada.pfw$date,unit=&quot;week&quot;)+6) We also assign region to the Canadian data set since several provinces are grouped for the purpose of summary statistics. canada.pfw &lt;- canada.pfw %&gt;% mutate(region = ifelse(Prov==&quot;ON&quot;, &quot;ON&quot;, ifelse (Prov==&quot;BC&quot;, &quot;BC&quot;, ifelse(Prov == &quot;AB&quot;, &quot;PR&quot;, ifelse(Prov==&quot;SK&quot;, &quot;PR&quot;, ifelse(Prov==&quot;MB&quot;, &quot;PR&quot;, ifelse(Prov==&quot;QC&quot;, &quot;QC&quot;, ifelse(Prov==&quot;NB&quot;, &quot;AT&quot;, ifelse(Prov==&quot;NS&quot;, &quot;AT&quot;, ifelse(Prov==&quot;PE&quot;, &quot;AT&quot;, ifelse(Prov==&quot;NL&quot;, &quot;AT&quot;, ifelse(Prov==&quot;YT&quot;, &quot;North&quot;, ifelse(Prov==&quot;NT&quot;, &quot;North&quot;, NA))))))))))))) Now, we need to remove invalid records as defined by Bonter and Greig 2021. Flagged observations are identied in the database as 0 in the VALID eld and their status in the review process is described using a combination of the VALID eld and the REVIEWED eld as dened here: VALID = 0; REVIEWED = 0; Interpretation: Observation triggered a ag by the automated system and awaits the review process. Note that such observations are removed. VALID = 0; REVIEWED = 1; Interpretation: Observation triggered a ag by the automated system and was reviewed; insucient evidence was provided to conrm the observation. Note that such observations are removed. VALID = 1; REVIEWED = 0; Interpretation: Observation did not trigger the automatic agging system and was accepted into the database without review. VALID = 1; REVIEWED = 1; Interpretation: Observation triggered the agging system and was approved by an expert reviewer. Based on these descriptions, we remove all VALID == 0, keep all VALID == 1. We do not use the REVIEW column at this time. canada.pfw &lt;- canada.pfw %&gt;% filter(valid==1) Step 3: Assign Species Before we proceed we are going to add some data columns that will help us with reporting. Specifically, we want to include the REPORT_AS, CATEGORY and PRIMARY_COM_NAME fields in the PFW_species_code.csv, which is part of the PFW Data Dictionary. This csv file is located in the Data folder on GitHub or can be accessed directly at the link provided. However, note that the csv file provided on Github has been cleaned to only retain the worksheet/ data columns we need. # Load the PFW species list sp&lt;-read.csv(&quot;Data/PFW_species_codes.csv&quot;) sp&lt;- sp %&gt;% dplyr::select(SPECIES_CODE, REPORT_AS, CATEGORY, PRIMARY_COM_NAME) # Join the tables by species_code canada.pfw&lt;-left_join(canada.pfw, sp, by=c(&quot;species_code&quot;=&quot;SPECIES_CODE&quot;)) Step 4: Save Data Now that you have processed the data that you want, you can save it locally in your Data folder. Since we process each raw data file in sequence, each will have its own unique file name. min.yr&lt;-min(canada.pfw$year) max.yr&lt;-max(canada.pfw$year) write.table(canada.pfw, file = paste(dat.dir,&quot;PFW_Canada_&quot;,min.yr,&quot;-&quot;,max.yr, &quot;.csv&quot;, sep=&quot;&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) Step 5: Start Again Assuming you want to work with more than one data file, start back at Step One and process the next raw data file. Once you are done processing your data, you can move to Chapter 3. "],["Zero3.html", "Chapter 3 Zero-fill Matrix 3.1 Load Filtered Data 3.2 Species Range 3.3 Sampling Events", " Chapter 3 Zero-fill Matrix Many analyses require that you have records not only of when/where species were detected, but also where they were not detected. PFW contains records of species presence, however, we can infer species absence when a species was not detected during a survey, if the survey was done within the bounds of the species range. Lets start this Chapter by reloading our libraries and resetting our working directories in the event you took a break and are starting a new session with your newly filtered and cleaned data set. require(tidyverse) require(reshape) require(data.table) require(tibble) require(raster) require(sp) require(rgeos) require(rgdal) out.dir &lt;- paste(&quot;Output/&quot;) dat.dir &lt;- paste(&quot;Data/&quot;) 3.1 Load Filtered Data First we will load each of the filtered and cleaned PFW dataset into your working Environment under a different dateframe name. #Old data dat1&lt;-fread(&quot;Data/PFW_Canada_1988-1995.csv&quot;) dat2&lt;-fread(&quot;Data/PFW_Canada_1995-2000.csv&quot;) dat3&lt;-fread(&quot;Data/PFW_Canada_2000-2005.csv&quot;) dat4&lt;-fread(&quot;Data/PFW_Canada_2005-2010.csv&quot;) dat5&lt;-fread(&quot;Data/PFW_Canada_2010-2015.csv&quot;) dat6&lt;-fread(&quot;Data/PFW_Canada_2015-2020.csv&quot;) #New data dat7&lt;-fread(&quot;Data/PFW_Canada_2020-2021.csv&quot;) Notice that the newest dateset provided by Cornell Labs is missing the plus_code (i.e., dat7 has one fewer column). We will add this column in the right spot to make the bind possible. dat7&lt;-add_column(dat7, plus_code = 0, .after = 12) Bind and write your full Canadian PFW dataset to the Data directory for use later in the analysis data&lt;-rbind(dat1, dat2, dat3, dat4, dat5, dat6, dat7) write.table(data, paste(dat.dir,&quot;PFW_Canada_All.csv&quot;, sep=&quot;&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) 3.2 Species Range We will zero-fill the data two different ways. Why? because one way is used for standardized reporting by Cornell and Birds Canada, and the second way is more biologically appropriate and should be adopted for research purposes. For reporting purposes, we will zero-fill the data matrix based on Province (or State). Specifically, if a species was detected &gt;10 time in the past 10 years within a specific Province, it will be considered in range. First, lets create the zero-fill range file for each Province. First, we creating a master list that links all loc_id from PFW to Province, the we determine if a species was detected &gt;=10 times. This is done using a loop, which creates a Range output table. Note that we use the REPORT_AS for the species ID field, not species_code (which would at first glance seems to make sense, but in the long run it doesnt work out. Trust me!). 3.2.1 Provinical Range Matrix # Using the past 10 years of data to assign range limits for species dat&lt;-data %&gt;% filter(year&gt;=2010) # Remove duplicated and NA master&lt;-dat %&gt;% dplyr::select(loc_id, latitude, longitude, Prov, region) %&gt;% distinct(loc_id, .keep_all=TRUE) %&gt;% filter(latitude != &quot;NA&quot;) %&gt;% filter(longitude != &quot;NA&quot;) # Write table to your output director for safe keeping. You will use the master list later. write.table(master, file = paste(out.dir, &quot;master_prov.csv&quot;, sep=&quot;&quot;), row.names = FALSE, sep = &quot;,&quot;) sp.list&lt;-unique(data$REPORT_AS) # n = 303 in this example # Create a species loop for(m in 1:length(sp.list)) { # m&lt;-1 #for testing each species sp.data &lt;-NULL sp.data &lt;- filter(dat, REPORT_AS == sp.list[m]) %&gt;% droplevels() # Count number of observation in each Atlas Block sp.data&lt;-sp.data %&gt;% group_by(Prov) %&gt;% dplyr::summarize(nobs=length(how_many)) %&gt;% mutate(count=ifelse(nobs&gt;=5, 1, 0)) %&gt;% dplyr::select(-nobs) colnames(sp.data)[colnames(sp.data) == &quot;count&quot;] &lt;- sp.list[m] #Optional: if there are less than 2 Atlas Blocks containing a record of a species, remove. Considered rare and/or out of range. # if(nrow(sp.data)&lt;2){ # sp.data&lt;-NULL # }else{ master&lt;-left_join(master, sp.data, by = &quot;Prov&quot; ) # } #end if statement } #end sp.list loop master[is.na(master)]=0 # Write your new range table to an output file write.table(master, file = paste(out.dir, &quot;Range_prov.csv&quot;, sep=&quot;&quot;), row.names = FALSE, sep = &quot;,&quot;) The output table you just created contains location information loc_id, latitude, logitude, Prov, and region and a column for each species id (i.e., REPORT_AS). Each row under the species id indicates if the Atlas Block is within range (1) or is out of range (0). 3.2.2 Atlas Block Range Matrix Now lets create the zero-fill range matrix in a more biologically meaningful way. We will define the species range a little more precisely using the Canadian Breeding Bird Atlas blocks (100 Ã— 100 km). This file is located in the Data folder on GitHub or can be requested directly from Birds Canada. If a species has been detected in an Atlas Block &gt;10 times in the past 10 years it will be considered in range. Lets start by creating a master list that links all loc_id from PFW to an Atlas Block code. This is done in several steps which are annotated in the code. # Loading the Atlas Blocks blocks &lt;- readOGR(dsn=&quot;Data&quot;, layer=&quot;NationalBlocks&quot;) # Remove duplicated from the master list master&lt;-data %&gt;% dplyr::select(loc_id, latitude, longitude, Prov, region) %&gt;% distinct(loc_id, .keep_all=TRUE) # Remove NAs pts&lt;-master %&gt;% filter(latitude != &quot;NA&quot;) %&gt;% filter(longitude != &quot;NA&quot;) # Make list into spatial coordinates pts &lt;- SpatialPoints(coords = cbind(pts$longitude, pts$latitude)) # Project to the appropriate datum proj4string(pts) &lt;- CRS(&quot;+proj=longlat +datum=WGS84 +no_defs&quot;) # Project points to be the same datum as the National Blocks layers pts_proj &lt;- spTransform(pts, CRS(&quot;+proj=longlat +datum=NAD83 +no_defs&quot;)) # Overlay the PFW points on Atlas blocks layer, and extract the block codes pts_blk &lt;- over(pts_proj, blocks[,&quot;BLOCK_ID_e&quot;]) # Assign the Atlas Block code back to the lat/long coordinate list master$BlockCode&lt;-pts_blk$BLOCK_ID_e # Make distinct again, first time may not have worked. master&lt;-master %&gt;% distinct(loc_id, .keep_all=TRUE) Next we assign the BCR codes to the master table for future use. # Loading the bcr polygon bcr &lt;- readOGR(dsn=&quot;Data&quot;, layer=&quot;bcrPolygon&quot;) # Remove duplicated coords&lt;-data %&gt;% dplyr::select(loc_id, latitude, longitude) %&gt;% distinct(loc_id, .keep_all = TRUE) # Remove NAs coords&lt;-coords %&gt;% filter(latitude != &quot;NA&quot;) %&gt;% filter(longitude != &quot;NA&quot;) # Make list into spatial coordinates points &lt;- SpatialPoints(coords = cbind(coords$longitude, coords$latitude)) # Project to the appropriate datum proj4string(points) &lt;- CRS(&quot;+proj=longlat +datum=WGS84 +no_defs&quot;) #Because some points fall outside the bcr polygon, we need to assign them to the nearest polygon # transform to sf objects psf &lt;- sf::st_as_sf(points) %&gt;% dplyr::mutate(ID_point = coords$loc_id) polsf &lt;- sf::st_as_sf(bcr) # remove points inside polygons sf::sf_use_s2(FALSE) # find nearest poly nearest &lt;- polsf[sf::st_nearest_feature(psf, polsf) ,] %&gt;% dplyr::mutate(id_point = psf$ID_point) # Assign the Atlas Block code back to the lat/long coordinate list coords$bcr&lt;-nearest$bcr # Merge back with the main data file coords&lt;-coords %&gt;% dplyr::select(-latitude, -longitude) master&lt;-left_join(master, coords, by=c(&quot;loc_id&quot;)) master&lt;-master %&gt;% mutate(bcr=ifelse(bcr==22|bcr==23, 13, bcr)) # Write table to your output director for safe keeping. You will use the master list later. write.table(master, file = paste(out.dir, &quot;master_blockcode.csv&quot;, sep=&quot;&quot;), row.names = FALSE, sep = &quot;,&quot;) Now we will use PFW data from the past 10 years to determine the species range. This is done in several steps which are annotated in the code. First we assign the National Atlas Block Codes. # Start by loading the Atlas Blocks if you haven&#39;t already blocks &lt;- readOGR(dsn=&quot;Data&quot;, layer=&quot;NationalBlocks&quot;) head(blocks) # Using the past 10 years of data to assign range limits for species dat&lt;-rbind(dat5, dat6, dat7) # Remove duplicated coords&lt;-dat %&gt;% dplyr::select(latitude, longitude) %&gt;% distinct() #keep distinct # Remove NAs coords&lt;-coords %&gt;% filter(latitude != &quot;NA&quot;) %&gt;% filter(longitude != &quot;NA&quot;) # Make list into spatial coordinates points &lt;- SpatialPoints(coords = cbind(coords$longitude, coords$latitude)) # Project to the appropriate datum proj4string(points) &lt;- CRS(&quot;+proj=longlat +datum=WGS84 +no_defs&quot;) # Project points to be the same datum as the National Blocks layers points_proj &lt;- spTransform(points, CRS(&quot;+proj=longlat +datum=NAD83 +no_defs&quot;)) # Overlay the PFW points on Atlas blocks layer, and extract the block codes points_blk &lt;- over(points_proj, blocks[,&quot;BLOCK_ID_e&quot;]) # Assign the Atlas Block code back to the lat/long coordinate list coords$BlockCode&lt;-points_blk$BLOCK_ID_e # Merge back with the main data file dat&lt;-merge(dat, coords, by=c(&quot;latitude&quot;, &quot;longitude&quot;)) Now that we have merged the PFW location data from the past 10 years with the Atlas Block codes, we can determine if a species was detected in a block &gt;=10 times. As before, this is done using a loop, which creates a Range output table. master&lt;-NULL master&lt;- read.csv(&quot;Output/master_blockcode.csv&quot;) sp.list&lt;-unique(data$REPORT_AS) # n = 303 in this example # Create a species loop for(m in 1:length(sp.list)) { # m&lt;-1 #for testing each species sp.data &lt;-NULL sp.data &lt;- filter(dat, REPORT_AS == sp.list[m]) %&gt;% droplevels() # Change to length # Count number of observation in each Atlas Block sp.data&lt;-sp.data %&gt;% group_by(BlockCode) %&gt;% dplyr::summarize(nobs=length(how_many)) %&gt;% mutate(count=ifelse(nobs&gt;=5, 1, 0)) %&gt;% dplyr::select(-nobs) colnames(sp.data)[colnames(sp.data) == &quot;count&quot;] &lt;- sp.list[m] #Optional: if there are less than 2 Atlas Blocks containing a record of a species, remove. Considered rare and/or out of range. # if(nrow(sp.data)&lt;2){ # sp.data&lt;-NULL # }else{ master&lt;-left_join(master, sp.data, by = &quot;BlockCode&quot; ) # } #end if statement } #end sp.list loop master[is.na(master)]=0 # Write your new range table to an output file write.table(master, file = paste(out.dir, &quot;Range_blockcode.csv&quot;, sep=&quot;&quot;), row.names = FALSE, sep = &quot;,&quot;) The output table you just created contains location information loc_id, latitude, logitude, Prov, region, BlockCode and bcr and a column for each species id (i.e., REPORT_AS). Each row under the species id indicates if the Atlas Block is within range (1) or is out of range (0). 3.3 Sampling Events Now that we have a handle on the winter distribution/range of each species, we need to know when each PFW site was sampled. This way we only add a zero-count for a site that was being watched. This is done for each sub_id which covers the two-day count period. First we create new effort fields. The first sums the number of half days the feeder site was watched (max = 4) and the second changes the effort hours into a factor. # Create the full data table. This step is repetitive. data&lt;-rbind(dat1, dat2, dat3, dat4, dat5, dat6, dat7) # Create effort days field (max = 4) data&lt;-data %&gt;% mutate(Effort_days = (day1_am +day1_pm +day2_am +day2_pm)/2) # Create effort hours field that is a factor data&lt;-data %&gt;% mutate(Effort_hrs = as.factor(effort_hrs_atleast)) levels(data$Effort_hrs)&lt;-c(&quot;0_1&quot;, &quot;1_4&quot;, &quot;4_8&quot;, &quot;&gt;8&quot;) Now we are ready to create the full sampling events layer using the filtered Canadian PFW dataset. event.data &lt;- data %&gt;% dplyr::select(loc_id, sub_id, latitude, longitude, month, day, year, Period, Effort_days, Effort_hrs, Prov) %&gt;% group_by(loc_id, sub_id, latitude, longitude, month, day, year, Period, Effort_days, Effort_hrs, Prov) %&gt;% distinct() %&gt;% ungroup() %&gt;% as.data.frame() # write your new sampling events table to an Output folder write.table(event.data, paste(out.dir,&quot;Events.csv&quot;, sep=&quot;&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) Now you have the tables that you need to zero-fill your data matrix. We can now start creating data summaries in Chapter 4. "],["Sum4.html", "Chapter 4 Summary Statistics 4.1 Average number of birds per week 4.2 Average number of species per week 4.3 Average number of species per site 4.4 Percent of sites with species at least once and mean group size 4.5 Top 10 Species 4.6 Plot Species Means", " Chapter 4 Summary Statistics Now that we have filtered and cleaned the data, and created the Range and sampling Events tables for zero-filling the data matrix, we are ready to start generating summary statistics. These summary statistics are generated annually to support Birds Canadas reporting for Canadian PFW participants. Lets start this Chapter by reloading our libraries and resetting our working directories in the event you took a break and are starting a new session with your newly filtered and cleaned data set. require(tidyverse) require(data.table) out.dir &lt;- paste(&quot;Output/&quot;) dat.dir &lt;- paste(&quot;Data/&quot;) # Load full dataset data&lt;-fread(&quot;Data/PFW_Canada_All.csv&quot;) Before we proceed we are going to add one more filter to the data. Specifically, there are some CATEGORY types that are not useful for our summary purposes (e.g., hybrids = slash &amp; integrade, unknown = spuh, undescribed = form, and domestic = domestic). If you want to keep all records, then skip this step. data&lt;-data %&gt;% filter(CATEGORY %in% c(&quot;species&quot;, &quot;issf&quot;)) %&gt;% droplevels() 4.1 Average number of birds per week Calculate the average number of individual birds per week at each station, summarized by region, province and nationally. Note that this is not a zero-filled data matrix. I assume that all checklists have a minimum of one bird recorded. This is likely a true assumption. # Region ind.week.reg&lt;-data %&gt;% group_by(region, Period, floor.week, loc_id) %&gt;% summarize(loc.sum=sum(how_many)) ind.week.reg&lt;-ind.week.reg %&gt;% group_by(region, Period) %&gt;% summarize(reg.ave=mean(loc.sum)) ind.week.region&lt;-cast(ind.week.reg, Period~region, value=&quot;reg.ave&quot;) write.table(format(ind.week.region, digits=4), file = paste(out.dir,&quot;Average number of individual birds per week_region.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # Province ind.week.prov&lt;-data %&gt;% group_by(Prov, Period, floor.week, loc_id) %&gt;% summarize(loc.sum=sum(how_many)) ind.week.prov&lt;-ind.week.prov %&gt;% group_by(Prov, Period) %&gt;% summarize(prov.ave=mean(loc.sum)) ind.week.province&lt;-cast(ind.week.prov, Period~Prov, value=&quot;prov.ave&quot;) write.table(format(ind.week.province, digits=4), file = paste(out.dir,&quot;Average number of individual birds per week_province.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # National ind.week.nat &lt;-data %&gt;% group_by(Period, floor.week, loc_id) %&gt;% summarize(loc.sum=sum(how_many)) ind.week.nat&lt;-ind.week.nat %&gt;% group_by(Period) %&gt;% summarize(nat.ave=mean(loc.sum)) ind.week.nat&lt;-as.data.frame((ind.week.nat)) write.table(format(ind.week.nat, digits=4), file = paste(out.dir,&quot;Average number of individual birds per week_national.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) 4.2 Average number of species per week Calculate the average number of species (REPORT_AS) per week at each station, summarized by region, province and nationally. Note that this is not a zero-filled data matrix. I assume that all checklists have a minimum of one bird recorded. This is likely a true assumption. # Region sp.week.reg&lt;-data %&gt;% group_by(region, Period, floor.week, loc_id) %&gt;% summarize(loc.sp=n_distinct(REPORT_AS)) sp.week.reg&lt;-sp.week.reg %&gt;% group_by(region, Period) %&gt;% summarize(reg.ave=mean(loc.sp)) sp.week.reg&lt;-cast(sp.week.reg, Period~region, value=&quot;reg.ave&quot;) write.table(format(sp.week.reg, digits=3), file = paste(out.dir,&quot;Average number of species per week_region.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # Province sp.week.prov&lt;-data %&gt;% group_by(Prov, Period, floor.week, loc_id) %&gt;% summarize(loc.sp=n_distinct(REPORT_AS)) sp.week.prov&lt;-sp.week.prov %&gt;% group_by(Prov, Period) %&gt;% summarize(prov.ave=mean(loc.sp)) sp.week.prov&lt;-cast(sp.week.prov, Period~Prov, value=&quot;prov.ave&quot;) write.table(format(sp.week.prov, digits=3), file = paste(out.dir,&quot;Average number of species per week_province.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # National sp.week.nat&lt;-data %&gt;% group_by(Period, floor.week, loc_id) %&gt;% summarize(loc.sp=n_distinct(REPORT_AS)) sp.week.nat&lt;-sp.week.nat %&gt;% group_by(Period) %&gt;% summarize(nat.ave=mean(loc.sp)) sp.week.nat&lt;-as.data.frame((sp.week.nat)) write.table(format(sp.week.nat, digits=3), file = paste(out.dir,&quot;Average number of species per week_national.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) These number are inflated because there are species in the list that are not strictly feeder birds. 4.3 Average number of species per site Calculate the average number of species (REPORT_AS) per site in total years (all weeks combined) summarized by region, province and nationally. Note that this is not a zero-filled data matrix. I assume that all checklists have a minimum of one bird recorded. This is likely a true assumption. # Region tot.sp&lt;-data %&gt;% group_by(loc_id, Period, region) %&gt;% summarise(mean.sp.site=n_distinct(REPORT_AS)) %&gt;% group_by(Period, region) %&gt;% summarise(mean.sp=mean(mean.sp.site)) tot.sp&lt;-cast(tot.sp, Period~region, value=&quot;mean.sp&quot;) write.table(format(tot.sp, digits=3), file = paste(out.dir,&quot;Average number of species per site in total years_region.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # Province tot.sp.prov&lt;-data %&gt;% group_by(loc_id, Period, Prov) %&gt;% summarise(mean.sp.site=n_distinct(REPORT_AS)) %&gt;% group_by(Period, Prov) %&gt;% summarise(mean.sp=mean(mean.sp.site)) tot.sp.prov&lt;-cast(tot.sp.prov, Period~Prov, value=&quot;mean.sp&quot;) write.table(format(tot.sp.prov, digits=3), file = paste(out.dir,&quot;Average number of species per site in total years_province.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # National tot.sp.nat&lt;-data %&gt;% group_by(loc_id, Period) %&gt;% summarise(mean.sp.site=n_distinct(REPORT_AS)) %&gt;% group_by(Period) %&gt;% summarise(mean.sp=mean(mean.sp.site)) tot.sp.nat&lt;-as.data.frame((tot.sp.nat)) write.table(format(tot.sp.nat, digits=3), file = paste(out.dir,&quot;Average number of species per site in total years_national.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) These numbers may be slightly inflated because there are species in the list that are not strictly feeder birds. 4.4 Percent of sites with species at least once and mean group size Now we will do something a little more complicated: calculate the percent of sites with species (REPORT_AS) at least once in a season and mean group size. This is calculated for species that occur at &gt; 10 feeder sites. This summary statistics needs to be done on a zero-fill data matrix to correct for the number of sites an individual could have been seen but was not. To do this, we will use the Range_prov or Range_blockcode table, and the sampling Events table (see Chapter 3). # Region # Create regional table outside loop per.site.region&lt;- as.data.frame(matrix(data = NA, nrow = 1, ncol = 5, byrow = FALSE, dimnames = NULL)) names(per.site.region) &lt;- c(&quot;Region&quot;, &quot;Period&quot;, &quot;MeanGroup&quot;, &quot;PercentSite&quot;, &quot;Species&quot;) write.table(per.site.region, file = paste(out.dir,&quot;% sites with species at least once in season and mean group size_region.csv&quot;), row.names = FALSE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # Read in your tables from the Output folder range&lt;-fread(&quot;Output/Range_prov.csv&quot;) events&lt;-fread(&quot;Output/Events.csv&quot;) sp.list&lt;-unique(data$REPORT_AS) # Regional species loop for(n in 1:length(sp.list)) { # n&lt;-1 #for testing each species events1&lt;-NULL #clear previous dataframe events1&lt;-events %&gt;% dplyr::select(loc_id, sub_id, day, month, year, Period)%&gt;% filter(year!=&quot;NA&quot;) sp.data &lt;-NULL #clear previous dataframe sp.data &lt;- filter(data, REPORT_AS == sp.list[n]) %&gt;% droplevels() if(nrow(sp.data)&gt;0){ #only continue if there is data min.year&lt;-min(sp.data$year) max.year&lt;-max(sp.data$year) # Filter events data using the sp.data min and max year events1&lt;-events1 %&gt;% filter(year&gt;=min.year &amp; year&lt;=max.year) # Merge sp.data to events data sp.data&lt;-dplyr::left_join(events1, sp.data, by=c(&quot;loc_id&quot;, &quot;sub_id&quot;, &quot;day&quot;, &quot;month&quot;, &quot;year&quot;, &quot;Period&quot;)) sp.data&lt;-sp.data %&gt;% filter(loc_id != &quot;NA&quot;) %&gt;% dplyr::select(-Prov, -region) range.data&lt;-NULL #clear previous dataframe range.data&lt;-try(range %&gt;% dplyr::select(Prov, loc_id, region, Prov, sp.list[n]), silent=T) if(class(range.data) !=&quot;try-error&quot;){ # Join tables sp.data&lt;-left_join(sp.data, range.data, by=&quot;loc_id&quot;) #remove blocks that should not be included in the zero count colnames(sp.data)[colnames(sp.data) == sp.list[n]] &lt;- &quot;sp&quot; sp.data&lt;-sp.data %&gt;% filter(sp&gt;=1) # Zero fill the &#39;how_many&#39; column sp.data$how_many[is.na(sp.data$how_many)] &lt;- 0 # Now we can start building the summary statistics # Determine the number of distinct locations and the mean group size each location # Filter the data to include only species that have at least once per site, and seen at &gt; 10 sites. per.site.region&lt;-sp.data %&gt;% group_by(region, Period) %&gt;% mutate(loc_id_sp = ifelse(how_many==0, NA, loc_id)) %&gt;% summarize(tot.site=n_distinct(loc_id), n.site.sp=n_distinct(loc_id_sp), sp.sum=sum(how_many), mean.group=mean(how_many)) %&gt;% filter(sp.sum &gt; 0) %&gt;% filter(tot.site&gt;=10) %&gt;% mutate(per.site=((n.site.sp-1)/tot.site)*100) %&gt;% #subtract 1 for NAs dplyr::select(-sp.sum, -n.site.sp, -tot.site) per.site.region$SpeciesCode&lt;-sp.list[n] per.site.region&lt;-as.data.frame(per.site.region) write.table(format(per.site.region, digits=3), file = paste(out.dir,&quot;% sites with species at least once in season and mean group size_region.csv&quot;), row.names = FALSE, col.name = FALSE, append = TRUE, quote = FALSE, sep = &quot;,&quot;) } # end try catch } # end if nrow&gt;0 } #end species loop ######################################## # Province # Create provincial table outside loop per.site.prov&lt;- as.data.frame(matrix(data = NA, nrow = 1, ncol = 5, byrow = FALSE, dimnames = NULL)) names(per.site.prov) &lt;- c(&quot;Prov&quot;, &quot;Period&quot;, &quot;MeanGroup&quot;, &quot;PercentSite&quot;, &quot;Species&quot;) write.table(per.site.prov, file = paste(out.dir,&quot;% sites with species at least once in season and mean group size_prov.csv&quot;), row.names = FALSE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) range&lt;-fread(&quot;Output/Range_prov.csv&quot;) events&lt;-fread(&quot;Output/Events.csv&quot;) sp.list&lt;-unique(data$REPORT_AS) # Provincial species loop for(n in 1:length(sp.list)) { # n&lt;-1 #for testing each species events1&lt;-NULL #clear previous dataframe events1&lt;-events %&gt;% dplyr::select(loc_id, sub_id, day, month, year, Period)%&gt;% filter(year!=&quot;NA&quot;) %&gt;% filter(month %in% c(12, 1, 2)) sp.data &lt;-NULL sp.data &lt;- filter(data, REPORT_AS == sp.list[n]) %&gt;% droplevels() if(nrow(sp.data)&gt;0){ #only continue if there is data min.year&lt;-min(sp.data$year) max.year&lt;-max(sp.data$year) # Filter events data using the sp.data min and max year events1&lt;-events1 %&gt;% filter(year&gt;=min.year &amp; year&lt;=max.year) # First merge to events data sp.data&lt;-dplyr::left_join(events1, sp.data, by=c(&quot;loc_id&quot;, &quot;sub_id&quot;, &quot;day&quot;, &quot;month&quot;, &quot;year&quot;, &quot;Period&quot;)) sp.data&lt;-sp.data %&gt;% filter(loc_id != &quot;NA&quot;) %&gt;% dplyr::select(-Prov, -region) range.data&lt;-NULL # Some oddly labeled species are not in the block list. We therefore need a trycath to catch these errors so that the loop doesn&#39;t get broken. range.data&lt;-try(range %&gt;% dplyr::select(Prov, loc_id, region, Prov, sp.list[n]), silent=TRUE) if(class(range.data) !=&quot;try-error&quot;){ # Join tables sp.data&lt;-left_join(sp.data, range.data, by=&quot;loc_id&quot;) # Remove blocks that should not be included in the zero count colnames(sp.data)[colnames(sp.data) == sp.list[n]] &lt;- &quot;sp&quot; sp.data&lt;-sp.data %&gt;% filter(sp&gt;=1) # Zero fill the &#39;how_many&#39; column sp.data$how_many[is.na(sp.data$how_many)] &lt;- 0 # Now we can start building the summary statistics # Determine the number of distinct locations and the mean group size each location per.site.prov&lt;-sp.data %&gt;% group_by(Prov, Period) %&gt;% mutate(loc_id_sp = ifelse(how_many==0, NA, loc_id)) %&gt;% summarize(tot.site=n_distinct(loc_id), n.site.sp=n_distinct(loc_id_sp), sp.sum=sum(how_many), mean.group=mean(how_many)) %&gt;% filter(sp.sum &gt; 0) %&gt;% filter(tot.site&gt;=10) %&gt;% mutate(per.site=((n.site.sp-1)/tot.site)*100) %&gt;% #subtract 1 for NAs dplyr::select(-sp.sum, -n.site.sp, -tot.site) per.site.prov$SpeciesCode&lt;-sp.list[n] per.site.prov&lt;-as.data.frame(per.site.prov) write.table(format(per.site.prov, digits=3), file = paste(out.dir,&quot;% sites with species at least once in season and mean group size_prov.csv&quot;), row.names = FALSE, col.name = FALSE, append = TRUE, quote = FALSE, sep = &quot;,&quot;) } #end try catch } # end if nrow&gt;0 } #end species loop ######################################## # National # Create provincial table outside loop per.site.nat&lt;- as.data.frame(matrix(data = NA, nrow = 1, ncol = 4, byrow = FALSE, dimnames = NULL)) names(per.site.nat) &lt;- c(&quot;Period&quot;, &quot;MeanGroup&quot;, &quot;PercentSite&quot;, &quot;Species&quot;) write.table(per.site.nat, file = paste(out.dir,&quot;% sites with species at least once in season and mean group size_nat.csv&quot;), row.names = FALSE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) range&lt;-fread(&quot;Output/Range_prov.csv&quot;) events&lt;-fread(&quot;Output/Events.csv&quot;) sp.list&lt;-unique(data$REPORT_AS) # Provincial species loop for(n in 1:length(sp.list)) { # n&lt;-1 #for testing each species events1&lt;-NULL #clear previous dataframe events1&lt;-events %&gt;% dplyr::select(loc_id, sub_id, day, month, year, Period)%&gt;% filter(year!=&quot;NA&quot;) sp.data &lt;-NULL sp.data &lt;- filter(data, REPORT_AS == sp.list[n]) %&gt;% droplevels() if(nrow(sp.data)&gt;0){ #only continue if there is data min.year&lt;-min(sp.data$year) max.year&lt;-max(sp.data$year) # Filter events data using the sp.data min and max year events1&lt;-events1 %&gt;% filter(year&gt;=min.year &amp; year&lt;=max.year) # First merge to events data sp.data&lt;-dplyr::left_join(events1, sp.data, by=c(&quot;loc_id&quot;, &quot;sub_id&quot;, &quot;day&quot;, &quot;month&quot;, &quot;year&quot;, &quot;Period&quot;)) sp.data&lt;-sp.data %&gt;% filter(loc_id != &quot;NA&quot;) %&gt;% dplyr::select(-Prov, -region) range.data&lt;-NULL range.data&lt;-try(range %&gt;% dplyr::select(Prov, loc_id, region, Prov, sp.list[n]), silent=T) if(class(range.data) !=&quot;try-error&quot;){ # Join tables sp.data&lt;-left_join(sp.data, range.data, by=&quot;loc_id&quot;) # Remove blocks that should not be included in the zero count colnames(sp.data)[colnames(sp.data) == sp.list[n]] &lt;- &quot;sp&quot; sp.data&lt;-sp.data %&gt;% filter(sp&gt;=1) # Zero fill the &#39;how_many&#39; column sp.data$how_many[is.na(sp.data$how_many)] &lt;- 0 # Now we can start building the summary statistics # Determine the number of distinct locations and the mean group size each location per.site.nat&lt;-sp.data %&gt;% group_by(Period) %&gt;% mutate(loc_id_sp = ifelse(how_many==0, NA, loc_id)) %&gt;% summarize(tot.site=n_distinct(loc_id), n.site.sp=n_distinct(loc_id_sp), sp.sum=sum(how_many), mean.group=mean(how_many)) %&gt;% filter(sp.sum &gt; 0) %&gt;% filter(tot.site&gt;=10) %&gt;% mutate(per.site=((n.site.sp-1)/tot.site)*100) %&gt;% #subtract 1 for NAs dplyr::select(-sp.sum, -n.site.sp, -tot.site) per.site.nat$SpeciesCode&lt;-sp.list[n] per.site.nat&lt;-as.data.frame(per.site.nat) write.table(format(per.site.nat, digits=3), file = paste(out.dir,&quot;% sites with species at least once in season and mean group size_nat.csv&quot;), row.names = FALSE, col.name = FALSE, append = TRUE, quote = FALSE, sep = &quot;,&quot;) } #end try catch } # end if nrow&gt;0 } #end species loop 4.5 Top 10 Species The last summary statistic we will derive is the Top 10 Species each year based on the % of feeders and the mean group size, separately. To do this, we will use the output tables from the previous analysis. # Load the tables if they are not in your working environment per.site.region&lt;-read.csv(&quot;Output/ % sites with species at least once in season and mean group size_region.csv&quot;) per.site.prov&lt;-read.csv(&quot;Output/ % sites with species at least once in season and mean group size_prov.csv&quot;) per.site.nat&lt;-read.csv(&quot;Output/ % sites with species at least once in season and mean group size_nat.csv&quot;) # Top 10 species based on % feeders # Region top10.per.reg&lt;-per.site.region %&gt;% dplyr::select(-MeanGroup) %&gt;% group_by(Period, Region) %&gt;% slice_max(order_by = PercentSite, n = 10) top10.per.reg&lt;-as.data.frame(top10.per.reg) write.table(format(top10.per.reg, digits=3), file = paste(out.dir,&quot;Top 10 species percent feeders_region.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # Prov top10.per.prov&lt;-per.site.prov %&gt;% dplyr::select(-MeanGroup) %&gt;% group_by(Period, Prov) %&gt;% slice_max(order_by = PercentSite, n = 25) top10.per.prov&lt;-as.data.frame(top10.per.prov) write.table(format(top10.per.prov, digits=3), file = paste(out.dir,&quot;Top 10 species percent feeders_prov.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # National top10.per.nat&lt;-per.site.nat %&gt;% dplyr::select(-MeanGroup) %&gt;% group_by(Period) %&gt;% slice_max(order_by = PercentSite, n = 10) top10.per.nat&lt;-as.data.frame(top10.per.nat) write.table(format(top10.per.nat, digits=3), file = paste(out.dir,&quot;Top 10 species percent feeders_nat.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # Top 10 species based on MeanGroup size # Region top10.mean.reg&lt;-per.site.region %&gt;% dplyr::select(-PercentSite) %&gt;% group_by(Period, Region) %&gt;% slice_max(order_by = MeanGroup, n = 10) top10.mean.reg&lt;-as.data.frame(top10.mean.reg) write.table(format(top10.mean.reg, digits=3), file = paste(out.dir,&quot;Top 10 species mean group feeders_region.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # Prov top10.mean.prov&lt;-per.site.prov %&gt;% dplyr::select(-PercentSite) %&gt;% group_by(Period, Prov) %&gt;% slice_max(order_by = MeanGroup, n = 10) top10.mean.prov&lt;-as.data.frame(top10.mean.prov) write.table(format(top10.mean.prov, digits=3), file = paste(out.dir,&quot;Top 10 species mean group feeders_prov.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) # National top10.mean.nat&lt;-per.site.nat %&gt;% dplyr::select(-PercentSite) %&gt;% group_by(Period) %&gt;% slice_max(order_by = MeanGroup, n = 10) top10.mean.nat&lt;-as.data.frame(top10.mean.nat) write.table(format(top10.mean.nat, digits=3), file = paste(out.dir,&quot;Top 10 species mean group feeders_nat.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) 4.6 Plot Species Means Now that we have generated the datatables, lets plot some data of interest. First, lets look at changes in the mean number of species over time. This can be done regionally, provincially, and/ or nationally with only slight changes to the code below. Also, you can simply change the filter to whichever species you want to plot over time. Notice that the REPORT_AS is now labled as Species in the output tables. # Load the tables if they are not in your working environment per.site.region&lt;-read.csv(&quot;Output/ % sites with species at least once in season and mean group size_region.csv&quot;) per.site.prov&lt;-read.csv(&quot;Output/ % sites with species at least once in season and mean group size_prov.csv&quot;) per.site.nat&lt;-read.csv(&quot;Output/ % sites with species at least once in season and mean group size_nat.csv&quot;) # Filter for the species of interest sp.mean&lt;-per.site.region %&gt;% filter(Species==&quot;amecro&quot;) # Plot the mean number ggplot(sp.mean, aes(Period, MeanGroup))+ geom_line(aes(colour=Region), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Mean number per site&quot;) # Plot the % feeders ggplot(sp.mean, aes(Period, PercentSite))+ geom_line(aes(colour=Region), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;% feeder sites&quot;) Next we are going to summarize effort in Chapter 5. "],["Effort5.html", "Chapter 5 Effort Statistics 5.1 Number of Sites Monitored 5.2 Number of Effort Days Monitored 5.3 Seasonal Effort 5.4 Supplemental Information 5.5 Housing and Population Denisty 5.6 Feeder Type", " Chapter 5 Effort Statistics Using the sampling Events tables we are going to generate some effort based summary statistics. These summary statistics are generated annually to support Birds Canadas reporting for Canadian PFW participants, and can be used to correct for effort in more involve statistical analyses. Lets start this Chapter by reload our libraries and resetting our working directories in the event you took a break and are staring a new session with your newly filtered and cleaned data set. require(tidyverse) require(reshape) out.dir &lt;- paste(&quot;Output/&quot;) dat.dir &lt;- paste(&quot;Data/&quot;) # Load datatables event&lt;-read.csv(&quot;Output/Events.csv&quot;) # Recall there are two range tables. One for Atlas blocks and one for Provinces blk&lt;-read.csv(&quot;Output/Range_blockcode.csv&quot;) We will merge the Events table with part of the Range table first so that we have all the information we need in one dataframe. blk&lt;-blk %&gt;% dplyr::select(loc_id, Prov, region, BlockCode) %&gt;% distinct(loc_id, .keep_all = TRUE) eff&lt;-left_join(event, blk, by=&quot;loc_id&quot;) 5.1 Number of Sites Monitored First we will summarize the number of loc_id monitored in a year, regionally, provincially, and nationally. We will also plot these outputs since it is easy and interesting to look at. loc_id_reg&lt;-eff %&gt;% group_by(region, Period) %&gt;% summarize (n_loc = n_distinct(loc_id)) %&gt;% drop_na() ## `summarise()` has grouped output by &#39;region&#39;. You can override using the `.groups` argument. ggplot(loc_id_reg, aes(x=Period, y=n_loc))+ geom_line(aes(colour=region), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Number of count sites (log)&quot;)+ ggtitle(&quot;Regional&quot;)+ scale_y_log10() loc_id_prov&lt;-eff %&gt;% group_by(Prov, Period) %&gt;% summarize (n_loc = n_distinct(loc_id)) %&gt;% drop_na() ## `summarise()` has grouped output by &#39;Prov&#39;. You can override using the `.groups` argument. ggplot(loc_id_prov, aes(x=Period, y=n_loc))+ geom_line(aes(colour=Prov), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Number of count sites (log)&quot;)+ ggtitle(&quot;Provincial&quot;)+ scale_y_log10() loc_id_nat&lt;-eff %&gt;% group_by(Period) %&gt;% summarize (n_loc = n_distinct(loc_id)) %&gt;% drop_na() ggplot(loc_id_nat, aes(x=Period, y=n_loc))+ geom_line(size=1.5)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Number of count sites&quot;)+ ggtitle(&quot;National&quot;)+ geom_point(aes(x=2020, y=2172), colour=&quot;red&quot;, size=5)+ annotate(&quot;text&quot;, x = 2017, y = 2200, label = &quot;COVID-19&quot;, colour=&quot;red&quot;, size = 5) write.table(format(loc_id_reg, digits=4), file = paste(out.dir,&quot;Site Effort_reg.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) write.table(format(loc_id_prov, digits=4), file = paste(out.dir,&quot;Site Effort_prov.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) write.table(format(loc_id_nat, digits=4), file = paste(out.dir,&quot;Site Effort_nat.csv&quot;), row.names = FALSE, col.name = TRUE, append = FALSE, quote = FALSE, sep = &quot;,&quot;) 5.2 Number of Effort Days Monitored Next, we can look at the number of half days monitored. This has been consistently recorded since 2014, so we will truncate the Period to only include dates later then this. days_reg&lt;-eff %&gt;% group_by(region, Period) %&gt;% summarize (n_days = sum(Effort_days)) %&gt;% drop_na() %&gt;% filter(Period&gt;=2014) ggplot(days_reg, aes(x=Period, y=n_days))+ geom_line(aes(colour=region), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Number of 1/2 count days (log)&quot;)+ ggtitle(&quot;Regional&quot;)+ scale_y_log10() days_prov&lt;-eff %&gt;% group_by(Prov, Period) %&gt;% summarize (n_days = sum(Effort_days)) %&gt;% drop_na() %&gt;% filter(Period&gt;=2014) ggplot(days_prov, aes(x=Period, y=n_days))+ geom_line(aes(colour=Prov), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Number of 1/2 count days (log)&quot;)+ ggtitle(&quot;Provinical&quot;)+ scale_y_log10() days_nat&lt;-eff %&gt;% group_by(Period) %&gt;% summarize (n_days = sum(Effort_days)) %&gt;% drop_na() %&gt;% filter(Period&gt;=2014) ggplot(days_nat, aes(x=Period, y=n_days))+ geom_line(size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Number of count 1/2 days (log)&quot;)+ ggtitle(&quot;National&quot;)+ scale_y_log10() 5.3 Seasonal Effort It might be interesting to look at seasonal changes in effort. For example, are more forms submitted at the beginning of the PFW season than the end? We can assess this using the sub_id, but will need to use the fully cleaned PFW data file for this task. # Load data data&lt;-read.csv(&quot;Data/PFW_Canada_All.csv&quot;) # Filter the dataframe dat&lt;-data %&gt;% dplyr::select(loc_id, Prov, sub_id, Period, floor.week) form_reg&lt;-dat %&gt;% group_by(Period, floor.week) %&gt;% summarize(n_form=n_distinct(sub_id)) %&gt;% drop_na() ggplot(form_reg, aes(x=floor.week, y=n_form))+ geom_line(group=1)+ theme_classic()+ theme(text=element_text(size=20), axis.text.x=element_blank())+ ylab(&quot;Number of forms submitted weekly&quot;) 5.4 Supplemental Information In addition to observation counts of birds at PFW locations, there is a single file containing supplementary information about the count locations (sites). This information is submitted by users. This file can be downloaded from the Cornell Lab PFW website. Once it is saved in your Data folder, we can begin exploring patterns in counts and habitat metrics. First, we will load the supplementary data and merge it to the site effort data previously compiled. #change this file name to match what is used in your Data folder hab&lt;-read.csv(&quot;Data/PFW_count_site_data_public_2021.csv&quot;) #separate the Period hab&lt;-hab %&gt;% separate(proj_period_id, c(&quot;del1&quot;, &quot;Period&quot;), sep=&quot;_&quot;, remove=FALSE) %&gt;% dplyr::select (-del1, -proj_period_id) hab$Period&lt;-as.integer(hab$Period) eff&lt;-left_join(eff, hab, by=c(&quot;loc_id&quot;, &quot;Period&quot;)) The FeederWatch_Data_Dictionary.xls, saved in the project working directory, will give you more information about each covariate in the Site Description Field Details tab. 5.5 Housing and Population Denisty Lets summarize changes in housing density of the neighborhood reported over time, where 1 = rural, 2 = rural/suburban, 3 = suburban, 4 = urban. house&lt;-eff %&gt;% select(loc_id, Period, housing_density) %&gt;% distinct() %&gt;% mutate(HouseDensity=ifelse(housing_density == 1, &quot;rural&quot;, ifelse(housing_density ==2, &quot;rural/suburban&quot;, ifelse(housing_density==3, &quot;suburban&quot;, ifelse(housing_density==4, &quot;urban&quot;, &quot;NA&quot;))))) house_sum&lt;-house%&gt;% group_by(Period, HouseDensity) %&gt;% summarise(n_sites=length(loc_id)) %&gt;% filter(HouseDensity!=&quot;NA&quot;) ggplot(house_sum, aes(x=Period, y=n_sites))+ geom_line(aes(colour=HouseDensity), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Number of site&quot;) #This can also be summarized by Province or Region house&lt;-eff %&gt;% select(loc_id, Period, region, housing_density) %&gt;% distinct() %&gt;% mutate(HouseDensity=ifelse(housing_density == 1, &quot;rural&quot;, ifelse(housing_density ==2, &quot;rural/suburban&quot;, ifelse(housing_density==3, &quot;suburban&quot;, ifelse(housing_density==4, &quot;urban&quot;, &quot;NA&quot;))))) house_sum&lt;-house%&gt;% group_by(Period, region, HouseDensity) %&gt;% summarise(n_sites=length(loc_id)) %&gt;% filter(HouseDensity!=&quot;NA&quot;) ggplot(house_sum, aes(x=Period, y=n_sites))+ geom_line(aes(colour=region), size=1)+ facet_wrap(~HouseDensity)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Number of site&quot;) Now lets summarize changes population at least, summarized over time, where 1 = less than 5,000; 5001 = 5,001 - 25,000; 25001 = 25,001 - 100,000; 100001 = &gt; 100,000. This will be compared visually to housing density to see if the two coincide. pop&lt;-eff %&gt;% select(loc_id, Period, population_atleast) %&gt;% distinct() %&gt;% mutate(PopDensity=ifelse(population_atleast == 1, &quot;&lt; 5000&quot;, ifelse(population_atleast == 5001, &quot;5000 - 25,000&quot;, ifelse(population_atleast==25001, &quot;25,000 - 100,000&quot;, ifelse(population_atleast==100001, &quot;&gt; 100,000&quot;, &quot;NA&quot;))))) pop_sum&lt;-pop%&gt;% group_by(Period, PopDensity) %&gt;% summarise(n_sites=length(loc_id)) %&gt;% filter(PopDensity!=&quot;NA&quot;) ggplot(pop_sum, aes(x=Period, y=n_sites))+ geom_line(aes(colour=PopDensity), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Number of site&quot;) #what if we rename these to be consistent with the housing density classifications pop&lt;-eff %&gt;% select(loc_id, Period, population_atleast) %&gt;% distinct() %&gt;% mutate(PopDensity=ifelse(population_atleast == 1, &quot;rural&quot;, ifelse(population_atleast == 5001, &quot;rural/suburban&quot;, ifelse(population_atleast==25001, &quot;suburban&quot;, ifelse(population_atleast==100001, &quot;urban&quot;, &quot;NA&quot;))))) pop_sum&lt;-pop%&gt;% group_by(Period, PopDensity) %&gt;% summarise(n_sites=length(loc_id)) %&gt;% filter(PopDensity!=&quot;NA&quot;) ggplot(pop_sum, aes(x=Period, y=n_sites))+ geom_line(aes(colour=PopDensity), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Number of site&quot;) 5.6 Feeder Type Another factor that could influence the types and number of birds counted is the feeder type and number. Let summarize those next. feed&lt;-eff %&gt;% select(loc_id, Period, region, numfeeders_suet, numfeeders_ground, numfeeders_hanging, numfeeders_hopper, numfeeders_platfrm, numfeeders_tube, numfeeders_other, numfeeders_thistle, numfeeders_water, numfeeders_humming, numfeeders_fruit) %&gt;% replace(is.na(.), 0) %&gt;% distinct() %&gt;% group_by(loc_id, Period) %&gt;% mutate(TotFeederType = sum(numfeeders_suet + numfeeders_ground + numfeeders_hanging + numfeeders_hopper + numfeeders_platfrm + numfeeders_tube +numfeeders_other + numfeeders_thistle + numfeeders_water + numfeeders_humming + numfeeders_fruit)) # Remove 0 from total counts. Assume this is just missing data. feed&lt;-feed %&gt;% filter(TotFeederType != 0) feed_sum&lt;-feed %&gt;% group_by(Period, region) %&gt;% summarise(MeanFeeder = mean(TotFeederType), TotalFeeder = sum(TotFeederType)) feed_sum&lt;-feed_sum %&gt;% filter(region!=&quot;North&quot;) ggplot(feed_sum, aes(x=Period, y=MeanFeeder))+ geom_line(aes(colour=region), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Mean number of feeders&quot;) #the total number of feeders is going to be correlated with the total number of sites being monitored ggplot(feed_sum, aes(x=Period, y=TotalFeeder))+ geom_line(aes(colour=region), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Total number of feeders&quot;) feed_sum&lt;-feed %&gt;% group_by(Period) %&gt;% summarise(suet = mean(numfeeders_suet), groud=mean(numfeeders_ground), hanging=mean(numfeeders_hanging), hopper=mean(numfeeders_hopper), platfrm = mean(numfeeders_platfrm), tube = mean(numfeeders_tube), thistle = mean(numfeeders_thistle), water = mean(numfeeders_water), humming= mean(numfeeders_humming), fruit = mean(numfeeders_fruit), other = mean(numfeeders_other)) #melt the dataframe for plotting #feed_sum&lt;-melt(feed_sum, id=&quot;Period&quot;, value.name = &quot;Mean&quot;) feed_sum&lt;-feed_sum %&gt;% tidyr::gather (&quot;FeederType&quot;, &quot;Mean&quot;, 2:12) feed_sum&lt;-feed_sum %&gt;% filter(Mean&gt;0) ggplot(feed_sum, aes(x=Period, y=Mean))+ geom_line(aes(colour=FeederType), size=1)+ theme_classic()+ theme(text=element_text(size=20))+ ylab(&quot;Mean number of feeders&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
